{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH =\"/home/samarthramesh/Work/Build-NNs/data/face_data/lfw-deepfunneled/lfw-deepfunneled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>/home/samarthramesh/Work/Build-NNs/data/face_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>/home/samarthramesh/Work/Build-NNs/data/face_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>/home/samarthramesh/Work/Build-NNs/data/face_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>/home/samarthramesh/Work/Build-NNs/data/face_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>/home/samarthramesh/Work/Build-NNs/data/face_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>/home/samarthramesh/Work/Build-NNs/data/face_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>/home/samarthramesh/Work/Build-NNs/data/face_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>/home/samarthramesh/Work/Build-NNs/data/face_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>/home/samarthramesh/Work/Build-NNs/data/face_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>/home/samarthramesh/Work/Build-NNs/data/face_d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         person                                               path\n",
       "0  Abdullah_Gul  /home/samarthramesh/Work/Build-NNs/data/face_d...\n",
       "1  Abdullah_Gul  /home/samarthramesh/Work/Build-NNs/data/face_d...\n",
       "2  Abdullah_Gul  /home/samarthramesh/Work/Build-NNs/data/face_d...\n",
       "3  Abdullah_Gul  /home/samarthramesh/Work/Build-NNs/data/face_d...\n",
       "4  Abdullah_Gul  /home/samarthramesh/Work/Build-NNs/data/face_d...\n",
       "5  Abdullah_Gul  /home/samarthramesh/Work/Build-NNs/data/face_d...\n",
       "6  Abdullah_Gul  /home/samarthramesh/Work/Build-NNs/data/face_d...\n",
       "7  Abdullah_Gul  /home/samarthramesh/Work/Build-NNs/data/face_d...\n",
       "8  Abdullah_Gul  /home/samarthramesh/Work/Build-NNs/data/face_d...\n",
       "9  Abdullah_Gul  /home/samarthramesh/Work/Build-NNs/data/face_d..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {\"person\" : [], \"path\" : []}\n",
    "for path in glob.iglob(os.path.join(DATASET_PATH, \"**\", \"*.jpg\")):\n",
    "    person = path.split(\"/\")[-2]\n",
    "    dataset[\"person\"].append(person)\n",
    "    dataset[\"path\"].append(path)\n",
    "    \n",
    "dataset = pd.DataFrame(dataset)\n",
    "# remove people with too many images\n",
    "# todo: randomly select which images to keep and discard excess\n",
    "dataset = dataset.groupby(\"person\").filter(lambda x: len(x) < 25 )\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_point = self.data_df.iloc[index]\n",
    "        name = data_point[\"person\"]\n",
    "        face = read_image(data_point[\"path\"])\n",
    "        return face, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(FaceDataset(dataset), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(train_dataloader))\n",
    "inputs = inputs/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Jennifer_Keller',\n",
       " 'Ajit_Agarkar',\n",
       " 'Catherine_Deneuve',\n",
       " 'Jennifer_Aniston',\n",
       " 'Martin_Bandier',\n",
       " 'Al_Leiter',\n",
       " 'Peter_OToole',\n",
       " 'Bruce_Van_De_Velde',\n",
       " 'Matt_Dillon',\n",
       " 'Tali_Imani',\n",
       " 'Zhu_Rongji',\n",
       " 'Katie_Harman',\n",
       " 'Robert_Duvall',\n",
       " 'Bono',\n",
       " 'Saddam_Hussein',\n",
       " 'Florencia_Kirchner',\n",
       " 'Rachel_Hunter',\n",
       " 'King_Gyanendra',\n",
       " 'Hank_Aaron',\n",
       " 'Jimmy_Lee',\n",
       " 'Winona_Ryder',\n",
       " 'Dale_Earnhardt_Jr',\n",
       " 'Paul_Murphy',\n",
       " 'Dave_Barr',\n",
       " 'Ronde_Barber',\n",
       " 'Roman_Abramovich',\n",
       " 'Keanu_Reeves',\n",
       " 'Anna_Kournikova',\n",
       " 'Audrey_Sauret',\n",
       " 'Eli_Rosenbaum',\n",
       " 'Hootie_Johnson',\n",
       " 'Natalie_Imbruglia')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.module.Module"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_output_res(in_channels, num_layers, kernel_size=3, stride=2, padding=1):\n",
    "    for _ in range(num_layers):\n",
    "        in_channels = np.floor(((in_channels-kernel_size+2*padding)/stride) + 1)\n",
    "    return int(in_channels)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dim, hidden_dims = None, kernel_size=3, stride=2, padding=1):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        if self.hidden_dims is None:\n",
    "            self.hidden_dims = [32, 64, 128, 256]\n",
    "\n",
    "        encoder_modules = []\n",
    "        for dim in range(len(self.hidden_dims) - 1):\n",
    "            encoder_modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        dim,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=padding\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(dim),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "            in_channels = dim\n",
    "\n",
    "\n",
    "        self.encoder_output_res = calc_output_res(in_channels, len(self.hidden_dims), kernel_size, stride, padding)\n",
    "        if self.encoder_output_res <= 0:\n",
    "            raise ValueError(\"Too Many Convolution Layers. Output Image is reduced to nothing.\")\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_modules)\n",
    "        self.fc_mean = nn.Linear(self.hidden_dims[-1]*(self.encoder_output_res**2), self.latent_dim)\n",
    "        self.fc_vars = nn.Linear(self.hidden_dims[-1]*(self.encoder_output_res**2), self.latent_dim)\n",
    "\n",
    "        reversed_dims = list(reversed(self.hidden_dims))\n",
    "        decoder_modules = []\n",
    "        for i in range(len(reversed_dims)-1):\n",
    "            decoder_modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        reversed_dims[i],\n",
    "                        reversed_dims[i+1],\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=padding,\n",
    "                        output_padding=padding\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(reversed_dims[i+1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(*decoder_modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                self.hidden_dims[-1],\n",
    "                self.hidden_dims[-1],\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                output_padding=padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.hidden_dims[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                hidden_dims[-1],\n",
    "                3,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        \n",
    "\n",
    "        \n",
    "    #     self.conv_layers = [\n",
    "    #         nn.Conv2d(hidden_dims[i], hidden_dims[i+1],\n",
    "    #                   kernel_size=3,\n",
    "    #                   stride=2,\n",
    "    #                   padding=1)\n",
    "    #                   for i in range(len(hidden_dims)-1)\n",
    "    #     ]\n",
    "\n",
    "    #     self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "    #     self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "    #     self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "    #     self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    #     self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "    #     self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "    #     self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "    #     self.deconv4 = nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    # def encoder(self, x):\n",
    "    #     x = F.relu(self.conv1(x))\n",
    "    #     x = F.relu(self.conv2(x))\n",
    "    #     x = F.relu(self.conv3(x))\n",
    "    #     x = F.relu(self.conv4(x))\n",
    "    #     x = x.view(-1, 256*14*14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = [1,2,3]\n",
    "thing.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing2 = list(reversed(thing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "124.0\n",
      "61.0\n",
      "30.0\n",
      "14.0\n",
      "6.0\n",
      "2.0\n",
      "0.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "input_size = 250\n",
    "kernel_size = 3\n",
    "padding = 0\n",
    "stride = 2\n",
    "n_layers = 10\n",
    "\n",
    "print(input_size)\n",
    "for i in range(n_layers):\n",
    "    input_size = np.floor(((input_size-kernel_size+2*padding)/stride) + 1)\n",
    "    print(input_size)\n",
    "\n",
    "# print((((input_size-kernel_size+2*padding)/stride) + 1))\n",
    "# print(np.floor(((input_size-kernel_size+2*padding)/stride) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 124, 124])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, 3, stride=2)\n",
    "conv(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0039, 0.0078, 0.0118, 0.0157, 0.0196, 0.0235, 0.0275, 0.0314,\n",
       "        0.0353, 0.0392, 0.0431, 0.0471, 0.0510, 0.0549, 0.0588, 0.0627, 0.0667,\n",
       "        0.0706, 0.0745, 0.0784, 0.0824, 0.0863, 0.0902, 0.0941, 0.0980, 0.1020,\n",
       "        0.1059, 0.1098, 0.1137, 0.1176, 0.1216, 0.1255, 0.1294, 0.1333, 0.1373,\n",
       "        0.1412, 0.1451, 0.1490, 0.1529, 0.1569, 0.1608, 0.1647, 0.1686, 0.1725,\n",
       "        0.1765, 0.1804, 0.1843, 0.1882, 0.1922, 0.1961, 0.2000, 0.2039, 0.2078,\n",
       "        0.2118, 0.2157, 0.2196, 0.2235, 0.2275, 0.2314, 0.2353, 0.2392, 0.2431,\n",
       "        0.2471, 0.2510, 0.2549, 0.2588, 0.2627, 0.2667, 0.2706, 0.2745, 0.2784,\n",
       "        0.2824, 0.2863, 0.2902, 0.2941, 0.2980, 0.3020, 0.3059, 0.3098, 0.3137,\n",
       "        0.3176, 0.3216, 0.3255, 0.3294, 0.3333, 0.3373, 0.3412, 0.3451, 0.3490,\n",
       "        0.3529, 0.3569, 0.3608, 0.3647, 0.3686, 0.3725, 0.3765, 0.3804, 0.3843,\n",
       "        0.3882, 0.3922, 0.3961, 0.4000, 0.4039, 0.4078, 0.4118, 0.4157, 0.4196,\n",
       "        0.4235, 0.4275, 0.4314, 0.4353, 0.4392, 0.4431, 0.4471, 0.4510, 0.4549,\n",
       "        0.4588, 0.4627, 0.4667, 0.4706, 0.4745, 0.4784, 0.4824, 0.4863, 0.4902,\n",
       "        0.4941, 0.4980, 0.5020, 0.5059, 0.5098, 0.5137, 0.5176, 0.5216, 0.5255,\n",
       "        0.5294, 0.5333, 0.5373, 0.5412, 0.5451, 0.5490, 0.5529, 0.5569, 0.5608,\n",
       "        0.5647, 0.5686, 0.5725, 0.5765, 0.5804, 0.5843, 0.5882, 0.5922, 0.5961,\n",
       "        0.6000, 0.6039, 0.6078, 0.6118, 0.6157, 0.6196, 0.6235, 0.6275, 0.6314,\n",
       "        0.6353, 0.6392, 0.6431, 0.6471, 0.6510, 0.6549, 0.6588, 0.6627, 0.6667,\n",
       "        0.6706, 0.6745, 0.6784, 0.6824, 0.6863, 0.6902, 0.6941, 0.6980, 0.7020,\n",
       "        0.7059, 0.7098, 0.7137, 0.7176, 0.7216, 0.7255, 0.7294, 0.7333, 0.7373,\n",
       "        0.7412, 0.7451, 0.7490, 0.7529, 0.7569, 0.7608, 0.7647, 0.7686, 0.7725,\n",
       "        0.7765, 0.7804, 0.7843, 0.7882, 0.7922, 0.7961, 0.8000, 0.8039, 0.8078,\n",
       "        0.8118, 0.8157, 0.8196, 0.8235, 0.8275, 0.8314, 0.8353, 0.8392, 0.8431,\n",
       "        0.8471, 0.8510, 0.8549, 0.8588, 0.8627, 0.8667, 0.8706, 0.8745, 0.8784,\n",
       "        0.8824, 0.8863, 0.8902, 0.8941, 0.8980, 0.9020, 0.9059, 0.9098, 0.9137,\n",
       "        0.9176, 0.9216, 0.9255, 0.9294, 0.9333, 0.9373, 0.9412, 0.9451, 0.9490,\n",
       "        0.9529, 0.9569, 0.9608, 0.9647, 0.9686, 0.9725, 0.9765, 0.9804, 0.9843,\n",
       "        0.9882, 0.9922, 0.9961, 1.0000])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
